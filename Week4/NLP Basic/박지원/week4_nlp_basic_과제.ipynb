{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 1\n",
        "\n",
        "### **Q. 각 모델이 충족하는 속성에 대해 아래 표를 O/X로 채워주세요.**\n",
        "\n",
        "📍5번째 속성은 **LSTM 기준으로** O/X 여부 판단해주세요 ! <br>\n",
        "📍정답은 과제 마감 다음날 (9월 11일 수요일)에 **노션-정규세션-NLP basic**에 업로드 예정\n",
        "\n",
        "\n",
        "> #### **속성 설명**\n",
        "1. Order matters : 입력 시퀀스의 순서 중요 여부\n",
        "2. Variable Length : 고정된 길이가 아닌 다양한 길이의 시퀀스를 처리할 수 있는지 여부\n",
        "3. Differentiable : 미분가능\n",
        "4. Pairwise encoding : 두 단어 사이의 관계를 표현\n",
        "5. Preserves long-term : 장기적인 의존성\n"
      ],
      "metadata": {
        "id": "9WW4t3iCYKuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|               | N-gram | RNN   | LSTM  | Transformer |\n",
        "|:-------------:|:------:|:-----:|:-----:|:-----------:|\n",
        "| Order matters |      O  |  O   | O     | O           |\n",
        "| Variable length |      |  O   | O     | O           |\n",
        "| Differentiable |       |  O   | O     | O           |\n",
        "| Pairwise encoding |    |     |      | O           |\n",
        "| Preserves long-term |  |     | O     | O           |\n"
      ],
      "metadata": {
        "id": "paUeOH0OYNU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제 2\n",
        "\n",
        "\n",
        "### 목표 : 독일어를 영어로 번역하는 모델 만들기\n",
        "독일어 문장을 입력하면 영어로 번역해주는 모델을 seq2seq로 구현해봅시다"
      ],
      "metadata": {
        "id": "14MthA8WYQev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.6.0"
      ],
      "metadata": {
        "id": "7PbwGzED6TIV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e3d7af-37f6-4131-d15b-7d5fc2aa362d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "id": "DPKSSHzQ6Uoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94bdf121-3830-4f96-b2c6-0e88dd666bb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import spacy\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "MJWiAS2yWutF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizers\n",
        "\n",
        "- 문장의 토큰화, 태깅 등의 전처리를 수행하기 위해 `spaCy` 라이브러리에서 영어와 독일어 전처리 모듈을 설치해줍니다.\n",
        "- 두 언어의 문장이 주어졌기 때문에 영어와 독일어 각각에 대해 전처리해주어야 합니다.\n"
      ],
      "metadata": {
        "id": "uHmIKputmJfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "l7ZtI5IXm7EG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시\n",
        "result = spacy_en.tokenizer(\"I am a student.\")\n",
        "\n",
        "for i, token in enumerate(result):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ],
      "metadata": {
        "id": "co1TC8yv7yrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcede0d8-65aa-443b-d16d-868e5173bf0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: student\n",
            "인덱스 4: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필드(field) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시해줍니다."
      ],
      "metadata": {
        "id": "lEGmP9Uk8gQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================\n",
        "# 💡 토큰화 결과가 list로 반환될 수 있도록 return 결과값을 채워주세요\n",
        "# seq2sxeq 논문에 의하면, input 단어의 순서를 바꾸면 최적화가 더 쉬워져 성능이 좋아진다고 합니다.\n",
        "# 💡 독일어 토큰화 결과가 역순으로 return될 수 있도록 반영해주세요!\n",
        "#===================================================\n",
        "def tokenize_de(text):\n",
        "    return [token.text for token in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "# 영어 문장을 spaCy를 사용해 토큰화하여 리스트로 반환\n",
        "def tokenize_en(text):\n",
        "    return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "uMjoI1XE7tGF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "필드(field) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시해줍니다."
      ],
      "metadata": {
        "id": "z3wGw1nPnpMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 독일어\n",
        "SRC = Field(tokenize= tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
        "# 영어\n",
        "TRG = Field(tokenize= tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True)"
      ],
      "metadata": {
        "id": "ubKI59GPpQ1f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 불러오기\n",
        "\n",
        "대표적인 영어-독어 번역 데이터셋 Multi30k을 불러옵니다.\n"
      ],
      "metadata": {
        "id": "0_ccI6_-8hR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/multi30k/dataset.git\n",
        "\n",
        "# 압축해제\n",
        "!gunzip /content/dataset/data/task1/raw/train.de.gz\n",
        "!gunzip /content/dataset/data/task1/raw/train.en.gz\n",
        "!gunzip /content/dataset/data/task1/raw/val.de.gz\n",
        "!gunzip /content/dataset/data/task1/raw/val.en.gz\n",
        "!gunzip /content/dataset/data/task1/raw/test_2018_flickr.de.gz\n",
        "!gunzip /content/dataset/data/task1/raw/test_2018_flickr.en.gz"
      ],
      "metadata": {
        "id": "mLA5kXAAf2uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbedb54-9186-4934-8462-f2f846825876"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 313 (delta 17), reused 21 (delta 16), pack-reused 281 (from 1)\u001b[K\n",
            "Receiving objects: 100% (313/313), 18.21 MiB | 8.49 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/dataset/data/task1/raw/'\n",
        "\n",
        "train_data = TranslationDataset(path=data_path, exts=('train.de', 'train.en'), fields=(SRC, TRG) )\n",
        "val_data = TranslationDataset(path=data_path, exts=('val.de', 'val.en'), fields=(SRC, TRG) )\n",
        "test_data = TranslationDataset(path=data_path, exts=('test_2018_flickr.de', 'test_2018_flickr.en'), fields=(SRC, TRG) )"
      ],
      "metadata": {
        "id": "M0JchGVU91Q5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"학습 데이터셋(training dataset) 크기: {len(train_data.examples)}개\")\n",
        "print(f\"평가 데이터셋(validation dataset) 크기: {len(val_data.examples)}개\")\n",
        "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_data.examples)}개\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxLylN1y-urW",
        "outputId": "fc914f75-0b89-400d-dde1-b760875b1625"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 데이터셋(training dataset) 크기: 29000개\n",
            "평가 데이터셋(validation dataset) 크기: 1014개\n",
            "테스트 데이터셋(testing dataset) 크기: 1071개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vars(train_data.examples[0]))\n",
        "print(vars(train_data.examples[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpt6l_Xd_AQX",
        "outputId": "a1bf82e8-7c99-490f-b70a-afc63498f6a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
            "{'src': ['.', 'antriebsradsystem', 'ein', 'bedienen', 'schutzhelmen', 'mit', 'männer', 'mehrere'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `build_vocab`함수를 이용하여 영어와 독일어의 단어 사전을 생성해줍니다. 이를 통해 각 token이 indexing됩니다\n",
        "- 단, vocabulary는 훈련 데이터셋에 대해서만 만들어져야 합니다.\n",
        "- `min_freq`를 사용하여 최소 2번 이상 나오는 단어들만 사전에 포함되도록 합니다."
      ],
      "metadata": {
        "id": "uNoigj40AD_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "metadata": {
        "id": "TfK-pAr_ApLP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
        "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
        "print(TRG.vocab.stoi[\"\"]) # : 0\n",
        "print(TRG.vocab.stoi[\"\"]) # : 0\n",
        "print(TRG.vocab.stoi[\"hello\"])\n",
        "print(TRG.vocab.stoi[\"world\"])"
      ],
      "metadata": {
        "id": "KyaA5P0Mrqaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1521ee-ea18-4ca7-fb93-fec068155c3c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "4112\n",
            "1752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 시퀀스 데이터는 각 문장의 길이가 다를 수 있습니다.\n",
        "- `BucketIterator는` 유사한 길이를 가진 샘플들을 같은 배치에 묶어주는 역할을 하기 때문에, 고정된 길이로 맞추기 위한 패딩의 양을 최소화할 수 있습니다."
      ],
      "metadata": {
        "id": "GmHXb1phBXJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "metadata": {
        "id": "uazI6xuv8rDH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 첫 번째 배치를 출력한 결과, [sequence length, batch size]라는 tensor가 생성됩니다\n",
        "- `sequence length`는 해당 배치 내에서 가장 긴 문장의 길이를 의미하며, 이보다 짧은 문장은 <pad> token으로 채워집니다.\n",
        "- 편의상 transpose한 뒤, 첫 번째와 두 번째 문장의 텐서를 출력하면, 특정 단어에 대응하는 인덱스가 출력되는 것을 알 수 있습니다.\n"
      ],
      "metadata": {
        "id": "Z154iai8Czsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    print(f\"첫 번째 배치의 text 크기: {src.shape}\")\n",
        "    src = src.transpose(1,0)\n",
        "    print(src[0])\n",
        "    print(src[1])\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "3r-eReL8rwm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60942270-1408-40fe-d9b5-86feef5c6529"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "첫 번째 배치의 text 크기: torch.Size([27, 128])\n",
            "tensor([   2,    4,    0,    8,  147,   37,   14,   10, 1477,   54,   76,    3,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1], device='cuda:0')\n",
            "tensor([   2,    4,   28, 1931,   59,    0,   44, 5136,  109,  182,  113,    8,\n",
            "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Seq2Seq with LSTM Model\n",
        "\n",
        "- seq2seq 이해를 위한 과제이니, 아래를 참고하여 작성해도 무방합니다 :)\n",
        "\n",
        "\n",
        "https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Sequence_to_Sequence_with_LSTM_Tutorial.ipynb"
      ],
      "metadata": {
        "id": "51xSGy35XLvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "i9WWS97vYnSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡아래줄에 embedding과 multi-layer LSTM 부분을 채워주세요 (dropout 포함)\n",
        "        #=========================================#\n",
        "        self.embedding =  nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [x length, batch size]\n",
        "        embedding = self.dropout(self.embedding(x))  # embedding = [x length, batch size, emb size]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # cell = [n layer, batch size, hid dim]\n",
        "        # outputs = [src len, batch size, hid dim]\n",
        "\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "GtpU_ZjeYNEZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ZrQoLPg-Ype1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡아래 코드를 채워주고, 각각 어떤 역할을 하는지 주석으로 간단히 설명해주세요\n",
        "        #\n",
        "        #=========================================#\n",
        "        # Embedding: 원-핫 인코딩말고 특정 차원의 임베딩, 즉 dense vector로 변환\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        # LSTM: 임베딩된 벡터를 입력받아 hidden state와 cell state를 처리\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=p)\n",
        "        # Fully connected layer: LSTM의 출력을 vocabulary 크기에 맞춰 예측값으로 변환\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        # 현재 input 형태 = [batch size]\n",
        "        # Decoder는 한번에 하나의 토큰만 처리하도록 sequence length = 1이 되어야 합니다\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(input))\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡self.rnn() 괄호 안 부분을 채워주세요\n",
        "        #=========================================#\n",
        "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "\n",
        "        prediction = self.fc(output.squeeze(0))  #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "NOVQfminYknh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq"
      ],
      "metadata": {
        "id": "NNsTqRamcfPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡trg를 사용하여 decoder에 입력할 첫번째 input을 설정해주세요\n",
        "        #=========================================#\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            # predictions들 중에 가장 잘 예측된 token 추출\n",
        "            best_guess = output.argmax(1) # [batch size]\n",
        "\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else best_guess\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Ik28Umx6gAPW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q. 위 코드에서는 매 시점마다 확률이 가장 높은 다음 단어를 선택하는 Greedy decoding  방식이 사용됩니다. 이런 방법을 채택할 경우 발생할 수 있는 문제점은 무엇일지 작성해주세요.**\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "# predictions들 중에 가장 잘 예측된 token 추출\n",
        "best_guess = output.argmax(1) # [batch size]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "➡️ Greedy decoding은 매 시점마다 가장 확률이 높은 단어를 선택하는 방식이다. 이 방식은 각 시점에서 최적의 선택을 하지만, 전체 문맥에서는 그 선택이 적합하지 않을 수 있다. 따라서 장기적인 최적의 결과를 놓칠 수 있다. 이로 인해 문법적으로 부적절하거나 자연스럽지 않은 문장이 생성될 가능성이 있다.\n"
      ],
      "metadata": {
        "id": "bfKrIZ63jkVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "EHk36-bkh055"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "metadata": {
        "id": "Xj33q4Izh3aB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 초기 가중치 값은 논문의 내용대로 U(−0.08,0.08)의 연속균등분포로부터 얻습니다."
      ],
      "metadata": {
        "id": "uoFprkiyh5vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "id": "-oLZ0jXyh4zG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b843da7f-8a3c-47d4-e3e6-25aa7be20c44"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(7853, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc): Linear(in_features=512, out_features=5893, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "TpKWiiBIF5NA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "veJrgUHBjRVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "LDfEPQUwFvVG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "O0fOgC3jc8K4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "5OCHWUhmGORD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "id": "n0VHYyZLjFO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b391c811-baad-446a-ac17-2725c5e3a363"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 45s\n",
            "\tTrain Loss: 5.049 | Train PPL: 155.842\n",
            "\t Val. Loss: 5.115 |  Val. PPL: 166.478\n",
            "Epoch: 02 | Time: 0m 44s\n",
            "\tTrain Loss: 4.465 | Train PPL:  86.952\n",
            "\t Val. Loss: 4.706 |  Val. PPL: 110.556\n",
            "Epoch: 03 | Time: 0m 42s\n",
            "\tTrain Loss: 4.143 | Train PPL:  62.999\n",
            "\t Val. Loss: 4.539 |  Val. PPL:  93.557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/tut1-model.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "id": "QCou2VSKVz64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bab1d4-08a4-4772-9ef5-c56c82347010"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-806c4c390395>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/tut1-model.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 5.133 | Test PPL: 169.522 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JsiX9L55ddft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}